{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Precondition\n",
    "To run the H20, download the latest version, and use java 11. Example of launch\n",
    "```shell\n",
    "path-to-java-11.exe -jar -Xmx16g -Xms8g  path-to-h2o.jar > NUL 2>&1\n",
    "```"
   ],
   "id": "9c7227a864a388e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:33:36.165176Z",
     "start_time": "2025-02-21T12:33:35.107591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# %%\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ass3.AutoMLClassifier import AutoMLClassifier\n",
    "\n",
    "\n",
    "def load_data(dataset_path, column_types_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    with open(column_types_path, 'rb') as feature_file:\n",
    "        feature_structure = pickle.load(feature_file)\n",
    "    return data, feature_structure\n",
    "\n",
    "\n",
    "def prepare_data(data, feature_structure):\n",
    "    \"\"\"Prepare data for training and testing: preprocess features and split data.\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Extract features and target\n",
    "    feature_columns = (\n",
    "            feature_structure['bin'] +\n",
    "            feature_structure['cat'] +\n",
    "            feature_structure['cont'] +\n",
    "            feature_structure['ord']\n",
    "    )\n",
    "    X = data[feature_columns]\n",
    "    y = data[feature_structure['target']]\n",
    "\n",
    "    # Check for missing values and clean the data\n",
    "    for col in feature_structure['bin'] + feature_structure['cat']:\n",
    "        # Ensure proper dtype conversion to 'category'\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "        # Replace missing values with placeholder (e.g., \"Unknown\")\n",
    "        data[col] = data[col].cat.add_categories(\"Unknown\").fillna(\"Unknown\")\n",
    "\n",
    "    for col in feature_structure['cont']:\n",
    "        # Fill missing continuous values with column mean (or another strategy)\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    for col in feature_structure['ord']:\n",
    "        # Ensure ordinal columns are numeric and have no missing values\n",
    "        data[col] = pd.to_numeric(data[col], errors=\"coerce\")  # Convert to numeric\n",
    "        data[col] = data[col].fillna(data[col].median())  # Fill missing with median\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    binary_and_cat_columns = feature_structure['bin'] + feature_structure['cat']\n",
    "    continuous_columns = feature_structure['cont']\n",
    "    ordinal_columns = feature_structure['ord']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"binary_cat\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "             binary_and_cat_columns),\n",
    "            (\"continuous\", StandardScaler(), continuous_columns),\n",
    "            (\"ordinal\", \"passthrough\", ordinal_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_processed = preprocessor.fit_transform(data[feature_columns])\n",
    "\n",
    "    # Convert the processed data back to DataFrame\n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=processed_feature_names)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed_df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test):\n",
    "    # # Train with TPOT\n",
    "    automl_classifier.train_tpot(X_train, y_train)\n",
    "    tpot_predictions = automl_classifier.predict_tpot(X_test)\n",
    "    tpot_accuracy = automl_classifier.evaluate(y_test, tpot_predictions)\n",
    "    print(f\"[+] TPOT Accuracy: {tpot_accuracy}\")\n",
    "\n",
    "    # Train with H2O AutoML\n",
    "    automl_classifier.train_h2o(X_train, y_train)\n",
    "    h2o_predictions = automl_classifier.predict_h2o(X_test)\n",
    "    h2o_accuracy = automl_classifier.evaluate(y_test, h2o_predictions)\n",
    "    print(f\"[+] H2O AutoML Accuracy: {h2o_accuracy}\")"
   ],
   "id": "f32e6ac6281b5420",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:44:54.279864Z",
     "start_time": "2025-02-21T12:33:36.184584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"dataset_path\": \"./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\",\n",
    "        \"column_types_path\": \"./data/breast_cancer/breast-cancer_column_types.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/alzheimer/alzheimers_prediction_dataset.csv\",\n",
    "        \"column_types_path\": \"./data/alzheimer/alzheimer_dataset.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/placement/placementdata.csv\",\n",
    "        \"column_types_path\": \"./data/placement/placement_metadata.pkl\",\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "automl_classifier = AutoMLClassifier()\n",
    "\n",
    "for dataset in datasets:\n",
    "    data, feature_structure = load_data(dataset[\"dataset_path\"], dataset[\"column_types_path\"])\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data, feature_structure)\n",
    "\n",
    "    print(f\"\\n Training on dataset: {dataset['dataset_path']}\")\n",
    "\n",
    "    train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test)\n"
   ],
   "id": "c1c9fdd19071e366",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on dataset: ./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\n",
      "[*] Training TPOT Classifier\n",
      "[+] Finished training TPOT Classifier\n",
      "[*] Making predictions with TPOT\n",
      "[+] TPOT Accuracy: 0.9824561403508771\n",
      "[*] Training H2O AutoML...\n",
      "Warning: Your H2O cluster version is (3 months and 19 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n",
      "Parse progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "13:34:34.236: AutoML: XGBoost is not available; skipping it.\n",
      "13:34:34.297: GBM_1_AutoML_6_20250221_133434 [GBM def_5] failed: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_6_20250221_133434.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 183.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 183.0.\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████ (cancelled)  95%\n"
     ]
    },
    {
     "ename": "H2OJobCancelled",
     "evalue": "Job<$0301c0a8009432d4ffffffff$_b67464f269032d1cc7fe1279145d48ea> was cancelled by the user.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mH2OJobCancelled\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 25\u001B[0m\n\u001B[0;32m     21\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m prepare_data(data, feature_structure)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Training on dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoml_classifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 90\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(automl_classifier, X_train, X_test, y_train, y_test)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[+] TPOT Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtpot_accuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# Train with H2O AutoML\u001B[39;00m\n\u001B[1;32m---> 90\u001B[0m \u001B[43mautoml_classifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_h2o\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m h2o_predictions \u001B[38;5;241m=\u001B[39m automl_classifier\u001B[38;5;241m.\u001B[39mpredict_h2o(X_test)\n\u001B[0;32m     92\u001B[0m h2o_accuracy \u001B[38;5;241m=\u001B[39m automl_classifier\u001B[38;5;241m.\u001B[39mevaluate(y_test, h2o_predictions)\n",
      "File \u001B[1;32m~\\IdeaProjects\\MachineLearning2024W\\ass3\\AutoMLClassifier.py:71\u001B[0m, in \u001B[0;36mAutoMLClassifier.train_h2o\u001B[1;34m(self, X_train, y_train)\u001B[0m\n\u001B[0;32m     67\u001B[0m predictors \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2o_model \u001B[38;5;241m=\u001B[39m H2OAutoML(max_runtime_secs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_time_h2o \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m60\u001B[39m,\n\u001B[0;32m     70\u001B[0m                            seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, balance_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mh2o_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpredictors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_frame\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mh2o_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[+] Finished training H2O AutoML\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\h2o\\automl\\_estimator.py:682\u001B[0m, in \u001B[0;36mH2OAutoML.train\u001B[1;34m(self, x, y, training_frame, fold_column, weights_column, validation_frame, leaderboard_frame, blending_frame)\u001B[0m\n\u001B[0;32m    680\u001B[0m poll_updates \u001B[38;5;241m=\u001B[39m ft\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll_training_updates, verbosity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verbosity, state\u001B[38;5;241m=\u001B[39m{})\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 682\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_job\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoll_updates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoll_updates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    684\u001B[0m     poll_updates(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_job, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\h2o\\job.py:85\u001B[0m, in \u001B[0;36mH2OJob.poll\u001B[1;34m(self, poll_updates)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# check if failed... and politely print relevant message\u001B[39;00m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANCELLED\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m H2OJobCancelled(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJob<\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m> was cancelled by the user.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjob_key)\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjob, \u001B[38;5;28mdict\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstacktrace\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjob)):\n",
      "\u001B[1;31mH2OJobCancelled\u001B[0m: Job<$0301c0a8009432d4ffffffff$_b67464f269032d1cc7fe1279145d48ea> was cancelled by the user."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:44:54.311987Z",
     "start_time": "2025-02-21T12:29:23.825116Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ea31ae0003460756",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
