{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:22:36.607706Z",
     "start_time": "2025-02-21T12:22:35.553293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# %%\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ass3.AutoMLClassifier import AutoMLClassifier\n",
    "\n",
    "\n",
    "def load_data(dataset_path, column_types_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    with open(column_types_path, 'rb') as feature_file:\n",
    "        feature_structure = pickle.load(feature_file)\n",
    "    return data, feature_structure\n",
    "\n",
    "\n",
    "def prepare_data(data, feature_structure):\n",
    "    \"\"\"Prepare data for training and testing: preprocess features and split data.\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Extract features and target\n",
    "    feature_columns = (\n",
    "            feature_structure['bin'] +\n",
    "            feature_structure['cat'] +\n",
    "            feature_structure['cont'] +\n",
    "            feature_structure['ord']\n",
    "    )\n",
    "    X = data[feature_columns]\n",
    "    y = data[feature_structure['target']]\n",
    "\n",
    "    # Check for missing values and clean the data\n",
    "    for col in feature_structure['bin'] + feature_structure['cat']:\n",
    "        # Ensure proper dtype conversion to 'category'\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "        # Replace missing values with placeholder (e.g., \"Unknown\")\n",
    "        data[col] = data[col].cat.add_categories(\"Unknown\").fillna(\"Unknown\")\n",
    "\n",
    "    for col in feature_structure['cont']:\n",
    "        # Fill missing continuous values with column mean (or another strategy)\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    for col in feature_structure['ord']:\n",
    "        # Ensure ordinal columns are numeric and have no missing values\n",
    "        data[col] = pd.to_numeric(data[col], errors=\"coerce\")  # Convert to numeric\n",
    "        data[col] = data[col].fillna(data[col].median())  # Fill missing with median\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    binary_and_cat_columns = feature_structure['bin'] + feature_structure['cat']\n",
    "    continuous_columns = feature_structure['cont']\n",
    "    ordinal_columns = feature_structure['ord']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"binary_cat\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "             binary_and_cat_columns),\n",
    "            (\"continuous\", StandardScaler(), continuous_columns),\n",
    "            (\"ordinal\", \"passthrough\", ordinal_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_processed = preprocessor.fit_transform(data[feature_columns])\n",
    "\n",
    "    # Convert the processed data back to DataFrame\n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=processed_feature_names)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed_df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test):\n",
    "    # # Train with TPOT\n",
    "    automl_classifier.train_tpot(X_train, y_train)\n",
    "    tpot_predictions = automl_classifier.predict_tpot(X_test)\n",
    "    tpot_accuracy = automl_classifier.evaluate(y_test, tpot_predictions)\n",
    "    print(f\"[+] TPOT Accuracy: {tpot_accuracy}\")\n",
    "\n",
    "    # Train with H2O AutoML\n",
    "    automl_classifier.train_h2o(X_train, y_train)\n",
    "    h2o_predictions = automl_classifier.predict_h2o(X_test)\n",
    "    h2o_accuracy = automl_classifier.evaluate(y_test, h2o_predictions)\n",
    "    print(f\"[+] H2O AutoML Accuracy: {h2o_accuracy}\")"
   ],
   "id": "f32e6ac6281b5420",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:29:23.772833Z",
     "start_time": "2025-02-21T12:22:36.611809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"dataset_path\": \"./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\",\n",
    "        \"column_types_path\": \"./data/breast_cancer/breast-cancer_column_types.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/alzheimer/alzheimers_prediction_dataset.csv\",\n",
    "        \"column_types_path\": \"./data/alzheimer/alzheimer_dataset.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/placement/placementdata.csv\",\n",
    "        \"column_types_path\": \"./data/placement/placement_metadata.pkl\",\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "automl_classifier = AutoMLClassifier()\n",
    "\n",
    "for dataset in datasets:\n",
    "    data, feature_structure = load_data(dataset[\"dataset_path\"], dataset[\"column_types_path\"])\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data, feature_structure)\n",
    "\n",
    "    print(f\"\\n Training on dataset: {dataset['dataset_path']}\")\n",
    "\n",
    "    train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test)\n"
   ],
   "id": "c1c9fdd19071e366",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on dataset: ./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\n",
      "[*] Training TPOT Classifier\n",
      "[+] Finished training TPOT Classifier\n",
      "[*] Making predictions with TPOT\n",
      "[+] TPOT Accuracy: 0.9824561403508771\n",
      "[*] Training H2O AutoML...\n",
      "Warning: Your H2O cluster version is (3 months and 19 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n",
      "Parse progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "13:23:33.822: AutoML: XGBoost is not available; skipping it.\n",
      "13:23:33.889: GBM_1_AutoML_3_20250221_132333 [GBM def_5] failed: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_3_20250221_132333.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 182.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 183.0.\n",
      "ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 183.0.\n",
      "\n",
      "\n",
      "███████████████████████████████████████████████████████████████| (done) 100%\n",
      "[+] Finished training H2O AutoML\n",
      "[*] Making predictions with H2O AutoML\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n",
      "[+] H2O AutoML Accuracy: 0.9824561403508771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n",
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on dataset: ./data/alzheimer/alzheimers_prediction_dataset.csv\n",
      "[*] Training TPOT Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:598: UserWarning: Skipping features without any observed values: ['ordinal__Physical Activity Level' 'ordinal__Social Engagement Level']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finished training TPOT Classifier\n",
      "[*] Making predictions with TPOT\n",
      "[+] TPOT Accuracy: 0.7181126741603284\n",
      "[*] Training H2O AutoML...\n",
      "Warning: Your H2O cluster version is (3 months and 19 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:598: UserWarning: Skipping features without any observed values: ['ordinal__Physical Activity Level' 'ordinal__Social Engagement Level']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "13:26:17.714: AutoML: XGBoost is not available; skipping it.\n",
      "13:26:17.717: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "\n",
      "13:26:18.618: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "██\n",
      "13:26:21.13: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "\n",
      "13:26:21.159: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "██████████\n",
      "13:26:29.298: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "13:26:30.643: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "███\n",
      "13:26:32.255: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "█\n",
      "13:26:34.339: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "13:26:34.609: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "13:26:34.885: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "██\n",
      "13:26:35.851: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "█\n",
      "13:26:37.84: _train param, Dropping bad and constant columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "███\n",
      "13:26:39.421: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "13:26:39.689: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "████████████████████████████████████\n",
      "13:27:13.478: _train param, Dropping unused columns: [ordinal__Social Engagement Level, ordinal__Physical Activity Level]\n",
      "\n",
      "█████| (done) 100%\n",
      "[+] Finished training H2O AutoML\n",
      "[*] Making predictions with H2O AutoML\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "[+] H2O AutoML Accuracy: 0.6707276031500303\n",
      "\n",
      " Training on dataset: ./data/placement/placementdata.csv\n",
      "[*] Training TPOT Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finished training TPOT Classifier\n",
      "[*] Making predictions with TPOT\n",
      "[+] TPOT Accuracy: 1.0\n",
      "[*] Training H2O AutoML...\n",
      "Warning: Your H2O cluster version is (3 months and 19 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "13:28:21.875: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "███████████████████████████████████████████████████████████████| (done) 100%\n",
      "[+] Finished training H2O AutoML\n",
      "[*] Making predictions with H2O AutoML\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "[+] H2O AutoML Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\IdeaProjects\\tu-ws-24-ml\\venv\\Lib\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:29:23.827684Z",
     "start_time": "2025-02-21T12:29:23.825116Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ea31ae0003460756",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
