{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7227a864a388e9",
   "metadata": {},
   "source": [
    "## Precondition\n",
    "To run the H20, download the latest version, and use java 11. Example of launch\n",
    "```shell\n",
    "path-to-java-11.exe -jar -Xmx16g -Xms8g  path-to-h2o.jar > NUL 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32e6ac6281b5420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:21:44.494028Z",
     "start_time": "2025-02-24T16:21:44.485839Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.src.metrics.accuracy_metrics import accuracy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# %%\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from ass3.AutoMLClassifier import AutoMLClassifier\n",
    "from BigDaddyWrapper import BigDaddyWrapper\n",
    "\n",
    "def load_data(dataset_path, column_types_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    with open(column_types_path, 'rb') as feature_file:\n",
    "        feature_structure = pickle.load(feature_file)\n",
    "    return data, feature_structure\n",
    "\n",
    "\n",
    "def prepare_data(data, feature_structure):\n",
    "    \"\"\"Prepare data for training and testing: preprocess features and split data.\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Extract features and target\n",
    "    feature_columns = (\n",
    "            feature_structure['bin'] +\n",
    "            feature_structure['cat'] +\n",
    "            feature_structure['cont'] +\n",
    "            feature_structure['ord']\n",
    "    )\n",
    "    X = data[feature_columns]\n",
    "    y = data[feature_structure['target']]\n",
    "\n",
    "    # Check for missing values and clean the data\n",
    "    for col in feature_structure['bin'] + feature_structure['cat']:\n",
    "        # Ensure proper dtype conversion to 'category'\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "        # Replace missing values with placeholder (e.g., \"Unknown\")\n",
    "        data[col] = data[col].cat.add_categories(\"Unknown\").fillna(\"Unknown\")\n",
    "\n",
    "    for col in feature_structure['cont']:\n",
    "        # Fill missing continuous values with column mean (or another strategy)\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    for col in feature_structure['ord']:\n",
    "        # Ensure ordinal columns are numeric and have no missing values\n",
    "        data[col] = pd.to_numeric(data[col], errors=\"coerce\")  # Convert to numeric\n",
    "        data[col] = data[col].fillna(data[col].median())  # Fill missing with median\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    binary_and_cat_columns = feature_structure['bin'] + feature_structure['cat']\n",
    "    continuous_columns = feature_structure['cont']\n",
    "    ordinal_columns = feature_structure['ord']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"binary_cat\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "             binary_and_cat_columns),\n",
    "            (\"continuous\", StandardScaler(), continuous_columns),\n",
    "            (\"ordinal\", \"passthrough\", ordinal_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_processed = preprocessor.fit_transform(data[feature_columns])\n",
    "\n",
    "    # Convert the processed data back to DataFrame\n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=processed_feature_names)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed_df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test):\n",
    "#     # # Train with TPOT\n",
    "#     automl_classifier.train_tpot(X_train, y_train)\n",
    "#     tpot_predictions = automl_classifier.predict_tpot(X_test)\n",
    "#     tpot_accuracy = automl_classifier.evaluate(y_test, tpot_predictions)\n",
    "#     print(f\"[+] TPOT Accuracy: {tpot_accuracy}\")\n",
    "\n",
    "#     # Train with H2O AutoML\n",
    "#     automl_classifier.train_h2o(X_train, y_train)\n",
    "#     h2o_predictions = automl_classifier.predict_h2o(X_test)\n",
    "#     h2o_accuracy = automl_classifier.evaluate(y_test, h2o_predictions)\n",
    "#     print(f\"[+] H2O AutoML Accuracy: {h2o_accuracy}\")\n",
    "\n",
    "#     # Train with Big Daddy\n",
    "\n",
    "def train_and_evaluate_big_daddy(data, feature_structure):\n",
    "    feature_columns = (\n",
    "            feature_structure['bin'] +\n",
    "            feature_structure['cat'] +\n",
    "            feature_structure['cont'] +\n",
    "            feature_structure['ord']\n",
    "    )\n",
    "    X = data[feature_columns]\n",
    "    y = data[feature_structure['target']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    wrapper = BigDaddyWrapper((X_train, y_train), feature_structure, 1)\n",
    "    wrapper.train_model()\n",
    "    predict = wrapper.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    print(f\"[+] Big Daddy Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9fdd19071e366",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-24T16:21:44.504620Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bin': ['ExtracurricularActivities', 'PlacementTraining'], 'cat': ['PlacementStatus'], 'ord': [], 'cont': ['CGPA', 'Internships', 'Projects', 'Workshops/Certifications', 'AptitudeTestScore', 'SoftSkillsRating', 'SSC_Marks', 'HSC_Marks'], 'target': 'PlacementStatus'}\n",
      "\n",
      " Training on dataset: ./data/placement/placementdata.csv\n",
      "Start with classifier KNeighborsClassifier()\n",
      "Grid len: 1600\n",
      "Iterations: 48.141711433740376\n",
      "Start SA with 48.141711433740376 iterations\n",
      "Finished 49 SA iterations with best_score=1.0\n",
      "Finished KNeighborsClassifier(algorithm='kd_tree', leaf_size=50, n_jobs=-1,\n",
      "                     n_neighbors=3), score: 1.0, time: 16.458972930908203s\n",
      "Start with classifier SVC()\n",
      "Grid len: 51200\n",
      "Iterations: 51.46974585736059\n",
      "Start SA with 51.46974585736059 iterations\n",
      "Finished 52 SA iterations with best_score=1.0\n",
      "Finished SVC(C=0.5, class_weight='balanced', coef0=2.0, degree=4, gamma='auto',\n",
      "    shrinking=False), score: 1.0, time: 6.472249984741211s\n",
      "Start with classifier DecisionTreeClassifier()\n",
      "Grid len: 931392\n",
      "Iterations: 220.61593706513545\n",
      "Start SA with 220.61593706513545 iterations\n",
      "Finished 221 SA iterations with best_score=1.0\n",
      "Finished DecisionTreeClassifier(class_weight='balanced', criterion='log_loss',\n",
      "                       max_depth=1, max_features='sqrt', max_leaf_nodes=20,\n",
      "                       min_impurity_decrease=0.02, min_samples_split=10,\n",
      "                       splitter='random'), score: 1.0, time: 9.735066652297974s\n",
      "Start with classifier RandomForestClassifier()\n",
      "Grid len: 4656960\n",
      "Iterations: 52.44243778391361\n",
      "Start SA with 52.44243778391361 iterations\n",
      "Finished 53 SA iterations with best_score=1.0\n",
      "Finished RandomForestClassifier(criterion='entropy', max_depth=10, max_leaf_nodes=30,\n",
      "                       min_impurity_decrease=0.03, min_samples_leaf=7,\n",
      "                       min_samples_split=20, n_estimators=150), score: 1.0, time: 8.518072843551636s\n",
      "Start with classifier MLPClassifier()\n",
      "Grid len: 5443200\n",
      "Iterations: 2.815553605163527\n",
      "Start SA with 2.815553605163527 iterations\n",
      "Finished 3 SA iterations with best_score=1.0\n",
      "Finished MLPClassifier(alpha=0.01, batch_size=50, hidden_layer_sizes=(50, 20, 20),\n",
      "              learning_rate='invscaling', learning_rate_init=0.01,\n",
      "              max_iter=1000, shuffle=False, solver='sgd'), score: 1.0, time: 7.781943321228027s\n",
      "================ Best classifier: KNeighborsClassifier(algorithm='kd_tree', leaf_size=50, n_jobs=-1,\n",
      "                     n_neighbors=3), params: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 50, 'metric': 'manhattan', 'n_jobs': -1}, score: 1.0 ================\n",
      "[+] Big Daddy Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    # {\n",
    "    #     \"dataset_path\": \"./data/congress_voting/CongressionalVotingID.shuf.lrn.csv\",\n",
    "    #     \"column_types_path\": \"./data/congress_voting/congressional-voting.pkl\",\n",
    "    # },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\",\n",
    "        \"column_types_path\": \"./data/breast_cancer/breast-cancer_column_types.pkl\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"dataset_path\": \"./data/alzheimer/alzheimers_prediction_dataset.csv\",\n",
    "    #     \"column_types_path\": \"./data/alzheimer/alzheimer_dataset.pkl\",\n",
    "    # },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/placement/placementdata.csv\",\n",
    "        \"column_types_path\": \"./data/placement/placement_metadata.pkl\",\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "# automl_classifier = AutoMLClassifier()\n",
    "\n",
    "for dataset in datasets:\n",
    "    data, feature_structure = load_data(dataset[\"dataset_path\"], dataset[\"column_types_path\"])\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data, feature_structure)\n",
    "    print(feature_structure)\n",
    "\n",
    "    print(f\"\\n Training on dataset: {dataset['dataset_path']}\")\n",
    "\n",
    "    # train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test)\n",
    "    train_and_evaluate_big_daddy(data, feature_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31ae0003460756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:17:01.860976700Z",
     "start_time": "2025-02-21T12:29:23.825116Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
