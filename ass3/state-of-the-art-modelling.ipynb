{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Precondition\n",
    "To run the H20, download the latest version, and use java 11. Example of launch\n",
    "```shell\n",
    "path-to-java-11.exe -jar -Xmx16g -Xms8g  path-to-h2o.jar > NUL 2>&1\n",
    "```"
   ],
   "id": "9c7227a864a388e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:21:44.494028Z",
     "start_time": "2025-02-24T16:21:44.485839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.src.metrics.accuracy_metrics import accuracy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# %%\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ass3.AutoMLClassifier import AutoMLClassifier\n",
    "from ass3.BigDaddyWrapper import BigDaddyWrapper\n",
    "\n",
    "\n",
    "def load_data(dataset_path, column_types_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    with open(column_types_path, 'rb') as feature_file:\n",
    "        feature_structure = pickle.load(feature_file)\n",
    "    return data, feature_structure\n",
    "\n",
    "\n",
    "def prepare_data(data, feature_structure):\n",
    "    \"\"\"Prepare data for training and testing: preprocess features and split data.\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Extract features and target\n",
    "    feature_columns = (\n",
    "            feature_structure['bin'] +\n",
    "            feature_structure['cat'] +\n",
    "            feature_structure['cont'] +\n",
    "            feature_structure['ord']\n",
    "    )\n",
    "    X = data[feature_columns]\n",
    "    y = data[feature_structure['target']]\n",
    "\n",
    "    # Check for missing values and clean the data\n",
    "    for col in feature_structure['bin'] + feature_structure['cat']:\n",
    "        # Ensure proper dtype conversion to 'category'\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "        # Replace missing values with placeholder (e.g., \"Unknown\")\n",
    "        data[col] = data[col].cat.add_categories(\"Unknown\").fillna(\"Unknown\")\n",
    "\n",
    "    for col in feature_structure['cont']:\n",
    "        # Fill missing continuous values with column mean (or another strategy)\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    for col in feature_structure['ord']:\n",
    "        # Ensure ordinal columns are numeric and have no missing values\n",
    "        data[col] = pd.to_numeric(data[col], errors=\"coerce\")  # Convert to numeric\n",
    "        data[col] = data[col].fillna(data[col].median())  # Fill missing with median\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    binary_and_cat_columns = feature_structure['bin'] + feature_structure['cat']\n",
    "    continuous_columns = feature_structure['cont']\n",
    "    ordinal_columns = feature_structure['ord']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"binary_cat\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False),\n",
    "             binary_and_cat_columns),\n",
    "            (\"continuous\", StandardScaler(), continuous_columns),\n",
    "            (\"ordinal\", \"passthrough\", ordinal_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_processed = preprocessor.fit_transform(data[feature_columns])\n",
    "\n",
    "    # Convert the processed data back to DataFrame\n",
    "    processed_feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=processed_feature_names)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed_df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test):\n",
    "    # # Train with TPOT\n",
    "    automl_classifier.train_tpot(X_train, y_train)\n",
    "    tpot_predictions = automl_classifier.predict_tpot(X_test)\n",
    "    tpot_accuracy = automl_classifier.evaluate(y_test, tpot_predictions)\n",
    "    print(f\"[+] TPOT Accuracy: {tpot_accuracy}\")\n",
    "\n",
    "    # Train with H2O AutoML\n",
    "    automl_classifier.train_h2o(X_train, y_train)\n",
    "    h2o_predictions = automl_classifier.predict_h2o(X_test)\n",
    "    h2o_accuracy = automl_classifier.evaluate(y_test, h2o_predictions)\n",
    "    print(f\"[+] H2O AutoML Accuracy: {h2o_accuracy}\")\n",
    "\n",
    "    # Train with Big Daddy\n",
    "\n",
    "def train_and_evaluate_big_daddy(data, feature_structure):\n",
    "    wrapper = BigDaddyWrapper(data, feature_structure)\n",
    "    wrapper.train_model()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data, feature_structure)\n",
    "    predict = wrapper.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predict)\n",
    "    print(f\"[+] Big Daddy Accuracy: {accuracy}\")\n"
   ],
   "id": "f32e6ac6281b5420",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-24T16:21:44.504620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"dataset_path\": \"./data/congress_voting/CongressionalVotingID.shuf.lrn.csv\",\n",
    "        \"column_types_path\": \"./data/congress_voting/congressional-voting.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/breast_cancer/breast-cancer-diagnostic.shuf.lrn.csv\",\n",
    "        \"column_types_path\": \"./data/breast_cancer/breast-cancer_column_types.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/alzheimer/alzheimers_prediction_dataset.csv\",\n",
    "        \"column_types_path\": \"./data/alzheimer/alzheimer_dataset.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"dataset_path\": \"./data/placement/placementdata.csv\",\n",
    "        \"column_types_path\": \"./data/placement/placement_metadata.pkl\",\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "automl_classifier = AutoMLClassifier()\n",
    "\n",
    "for dataset in datasets:\n",
    "    data, feature_structure = load_data(dataset[\"dataset_path\"], dataset[\"column_types_path\"])\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data, feature_structure)\n",
    "\n",
    "    print(f\"\\n Training on dataset: {dataset['dataset_path']}\")\n",
    "\n",
    "    train_and_evaluate(automl_classifier, X_train, X_test, y_train, y_test)\n",
    "    train_and_evaluateBigDaddy(data, feature_structure)\n"
   ],
   "id": "c1c9fdd19071e366",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on dataset: ./data/congress_voting/CongressionalVotingID.shuf.lrn.csv\n",
      "[*] Training TPOT Classifier\n",
      "[+] Finished training TPOT Classifier\n",
      "[*] Best pipeline configuration found by TPOT:\n",
      "Pipeline(steps=[('extratreesclassifier',\n",
      "                 ExtraTreesClassifier(criterion='entropy',\n",
      "                                      max_features=0.35000000000000003,\n",
      "                                      min_samples_leaf=18, min_samples_split=9,\n",
      "                                      random_state=42))])\n",
      "[*] Making predictions with TPOT\n",
      "[+] TPOT Accuracy: 1.0\n",
      "[*] Training H2O AutoML...\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n",
      "Warning: Your H2O cluster version is (3 months and 22 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         6 mins 05 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.6\n",
       "H2O_cluster_version_age:    3 months and 22 days\n",
       "H2O_cluster_name:           H2O_from_python_Dmytro_3r8fig\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    14.57 Gb\n",
       "H2O_cluster_total_cores:    32\n",
       "H2O_cluster_allowed_cores:  32\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.12.0 final\n",
       "--------------------------  -----------------------------"
      ],
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>6 mins 05 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.6</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 months and 22 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Dmytro_3r8fig</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>14.57 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>32</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>32</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.12.0 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "17:23:00.277: AutoML: XGBoost is not available; skipping it.\n",
      "17:23:00.365: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 174.0.\n",
      "\n",
      "████████████████████████████████████████████████████"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:17:01.860976700Z",
     "start_time": "2025-02-21T12:29:23.825116Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ea31ae0003460756",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
